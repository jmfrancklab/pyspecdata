{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 1D BRD regularization\n\nfor 1D BRD, adapted mainly from Venkataramanan 2002\nbut checked against BRD 1981\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pylab import *\nfrom pyspecdata import *\nfrom scipy.optimize import nnls\nfrom numpy.random import seed\nseed(1234)\ninit_logging('debug')\nvd_list = nddata(linspace(5e-4,10,25),'vd')\nt1_name = r'$\\log(T_1)$'\nlogT1 = nddata(r_[-4:2:100j],t1_name)\ndef Gaussian_1d(axis,mu1,sigma1):\n    this_G = exp(-(axis-mu1)**2/2/sigma1**2)\n    return this_G\ntrue_F = Gaussian_1d(logT1.C.run(lambda x: 10**(x)),6,0.3)\n\n\nK = (1.-2*exp(-vd_list/10**(logT1)))\n\nK.reorder('vd') # make sure vd along rows\nprint(shape(K))\nprint(shape(true_F))\n\nM = K @ true_F # the fake data\nprint(shape(M))\n#M.setaxis('vd',y_axis)\nM.add_noise(0.2)\n\n# this is here to test the integrated 1D-BRD (for pyspecdata)\nprint(\"*** *** ***\")\nprint(ndshape(M))\nprint(ndshape(logT1))\nprint(\"*** *** ***\")\nsolution = M.C.nnls('vd',logT1, lambda x,y: 1-2*exp(-x/10**(y)), l='BRD')\n\ndef nnls_reg(K,b,val):\n    b_prime = r_[b,zeros(K.shape[1])]\n    x,_ = nnls(A_prime(K,val),b_prime)\n    return x\n\n# generate the A matrix, which should have form of the original kernel\n# and then an additional length corresponding to size of the data dimension, where smothing param val is placed \ndef A_prime(K,val):\n    dimension = K.shape[1]\n    A_prime = r_[K,val*eye(dimension)]\n    return A_prime\n\nplot_Lcurve = True\n#{{{ L-curve\nl = sqrt(logspace(-10,1,25)) # adjusting the left number will adjust the right side of L-curve\n\ndef vec_lcurve(l):\n    return M.real.C.nnls('vd',\n            logT1,lambda x,y: (1.-2*exp(-x/10**(y))), l=l)\n\n# solution matrix for l different lambda values\nx = vec_lcurve(l)\nprint(ndshape(x))\n# norm of the residual (data - soln)\nr_norm = x.get_prop('nnls_residual').data\n# norm of the solution (taken along the fit axis)\nx_norm = x.C.run(linalg.norm,t1_name).data\n\n# From L-curve\nthis_L = 0.226\n\nif plot_Lcurve:\n    # Next plot the L-curve\n    figure();title('L-Curve')\n    # I do not actually know why we take the log, but this is important for the shape\n    plot(log10(r_norm[:]),log10(x_norm[:]),'.')\n    annotate_plot = True\n    show_lambda = True\n    if annotate_plot:\n        if show_lambda:\n            for j,this_l in enumerate(l):\n                annotate('%0.4f'%this_l, (log10(r_norm[j]),log10(x_norm[j])),\n                         ha='left',va='bottom',rotation=45)\n        else:\n            for j,this_l in enumerate(l):\n                annotate('%d'%j, (log10(r_norm[j]),log10(x_norm[j])),\n                         ha='left',va='bottom',rotation=45)\n#}}}\n\n# generate data vector for smoothing\n\nprint(K.shape)\nL_opt_vec = nnls_reg(K.data,M.data.squeeze(),this_L)\n\nfigure();title('ILT distributions')\nL_opt_vec = nddata(L_opt_vec,t1_name).copy_axes(true_F)\nplot(true_F,label='True')\nprint(\"true mean:\",true_F.C.mean(t1_name).item(),\"\u00b1\",true_F.run(std,t1_name).item())\nplot(L_opt_vec,label='L-Curve')\nprint(\"opt. \u03bb mean:\",L_opt_vec.C.mean(t1_name).item(),\"\u00b1\",L_opt_vec.run(std,t1_name).item())\nplot(solution,':',label='pyspecdata-BRD')\nprint(\"BRD mean:\",solution.C.mean(t1_name).item(),\"\u00b1\",solution.run(std,t1_name).item())\nlegend()\nshow()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}